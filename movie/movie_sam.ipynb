{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e7R6c60ULivi",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-01-02T16:24:45.425892Z",
     "start_time": "2023-01-02T16:24:40.907732Z"
    },
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 3236,
     "status": "ok",
     "timestamp": 1666692047388,
     "user": {
      "displayName": "蕭登方",
      "userId": "04110943827274540537"
     },
     "user_tz": -480
    },
    "id": "e7R6c60ULivi",
    "outputId": "20995799-5d67-4618-ffbc-cf2af763f909"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: transformers in c:\\programdata\\anaconda3\\envs\\tensorflow\\lib\\site-packages (4.23.1)\n",
      "Requirement already satisfied: numpy>=1.17 in c:\\programdata\\anaconda3\\envs\\tensorflow\\lib\\site-packages (from transformers) (1.21.5)\n",
      "Requirement already satisfied: pyyaml>=5.1 in c:\\programdata\\anaconda3\\envs\\tensorflow\\lib\\site-packages (from transformers) (6.0)\n",
      "Requirement already satisfied: tokenizers!=0.11.3,<0.14,>=0.11.1 in c:\\programdata\\anaconda3\\envs\\tensorflow\\lib\\site-packages (from transformers) (0.13.1)\n",
      "Requirement already satisfied: regex!=2019.12.17 in c:\\programdata\\anaconda3\\envs\\tensorflow\\lib\\site-packages (from transformers) (2022.7.9)\n",
      "Requirement already satisfied: filelock in c:\\programdata\\anaconda3\\envs\\tensorflow\\lib\\site-packages (from transformers) (3.6.0)\n",
      "Requirement already satisfied: packaging>=20.0 in c:\\programdata\\anaconda3\\envs\\tensorflow\\lib\\site-packages (from transformers) (21.3)\n",
      "Requirement already satisfied: requests in c:\\programdata\\anaconda3\\envs\\tensorflow\\lib\\site-packages (from transformers) (2.28.1)\n",
      "Requirement already satisfied: tqdm>=4.27 in c:\\programdata\\anaconda3\\envs\\tensorflow\\lib\\site-packages (from transformers) (4.64.1)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.10.0 in c:\\programdata\\anaconda3\\envs\\tensorflow\\lib\\site-packages (from transformers) (0.10.1)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in c:\\programdata\\anaconda3\\envs\\tensorflow\\lib\\site-packages (from huggingface-hub<1.0,>=0.10.0->transformers) (4.3.0)\n",
      "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in c:\\programdata\\anaconda3\\envs\\tensorflow\\lib\\site-packages (from packaging>=20.0->transformers) (3.0.9)\n",
      "Requirement already satisfied: colorama in c:\\programdata\\anaconda3\\envs\\tensorflow\\lib\\site-packages (from tqdm>=4.27->transformers) (0.4.5)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in c:\\programdata\\anaconda3\\envs\\tensorflow\\lib\\site-packages (from requests->transformers) (1.26.12)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\programdata\\anaconda3\\envs\\tensorflow\\lib\\site-packages (from requests->transformers) (2022.6.15)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\programdata\\anaconda3\\envs\\tensorflow\\lib\\site-packages (from requests->transformers) (3.4)\n",
      "Requirement already satisfied: charset-normalizer<3,>=2 in c:\\programdata\\anaconda3\\envs\\tensorflow\\lib\\site-packages (from requests->transformers) (2.0.4)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b125ccb5",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-01-02T16:24:47.284033Z",
     "start_time": "2023-01-02T16:24:45.427870Z"
    },
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 104,
     "referenced_widgets": [
      "caac41f66b0142e28added42102acc2b",
      "cc50d7437a1b434683d60275b51a16b7",
      "64dc892c4b3645fcb2a3ba97f06e032c",
      "8dfa31321da04c048f4be0b325369ca5",
      "40937cf4c5b140cb996312e4619444db",
      "a542890fb452487e8eb6ea03b000ab33",
      "99457cf987d347368ed223bab27555c6",
      "e78290c248d741ec8a503a37b24aa143",
      "0a90c336520a4b37a07310ee07752420",
      "be822330c6c04df1ac3699bbf3a2760c",
      "9c06c8fbf24647908c33f7e522429777"
     ]
    },
    "executionInfo": {
     "elapsed": 858,
     "status": "ok",
     "timestamp": 1666692048242,
     "user": {
      "displayName": "蕭登方",
      "userId": "04110943827274540537"
     },
     "user_tz": -480
    },
    "id": "b125ccb5",
    "outputId": "3411176b-5711-41d9-9d49-0a8a4142ddf9"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import transformers\n",
    "from transformers import BertTokenizer\n",
    "from IPython.display import clear_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "cc8cb462",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-01-02T16:24:58.280573Z",
     "start_time": "2023-01-02T16:24:47.287035Z"
    },
    "executionInfo": {
     "elapsed": 497,
     "status": "ok",
     "timestamp": 1666692048736,
     "user": {
      "displayName": "蕭登方",
      "userId": "04110943827274540537"
     },
     "user_tz": -480
    },
    "id": "cc8cb462"
   },
   "outputs": [],
   "source": [
    "PRETRAINED_MODEL_NAME = \"ckiplab/bert-base-chinese\"  # 指定繁簡中文 BERT-BASE 預訓練模型\n",
    "\n",
    "from transformers import (\n",
    "   BertTokenizerFast,\n",
    "   AutoModelForMaskedLM,\n",
    "   AutoModelForCausalLM,\n",
    "   AutoModelForTokenClassification,\n",
    "   AutoModel,\n",
    ")\n",
    "\n",
    "\n",
    "tokenizer = BertTokenizerFast.from_pretrained(PRETRAINED_MODEL_NAME)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d139cf40",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-01-02T16:24:58.296521Z",
     "start_time": "2023-01-02T16:24:58.282522Z"
    },
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 4,
     "status": "ok",
     "timestamp": 1666692048737,
     "user": {
      "displayName": "蕭登方",
      "userId": "04110943827274540537"
     },
     "user_tz": -480
    },
    "id": "d139cf40",
    "outputId": "12e5aa8f-514c-47d1-cf24-98cf7bf25a1c"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "字典大小： 21128\n"
     ]
    }
   ],
   "source": [
    "vocab = tokenizer.vocab\n",
    "print(\"字典大小：\", len(vocab))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "24720ca8",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-01-02T16:25:04.432795Z",
     "start_time": "2023-01-02T16:24:58.299522Z"
    },
    "executionInfo": {
     "elapsed": 3,
     "status": "ok",
     "timestamp": 1666692048737,
     "user": {
      "displayName": "蕭登方",
      "userId": "04110943827274540537"
     },
     "user_tz": -480
    },
    "id": "24720ca8"
   },
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ojrXBYUQL45G",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-01-02T16:25:04.448757Z",
     "start_time": "2023-01-02T16:25:04.434753Z"
    },
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 2726,
     "status": "ok",
     "timestamp": 1666692051460,
     "user": {
      "displayName": "蕭登方",
      "userId": "04110943827274540537"
     },
     "user_tz": -480
    },
    "id": "ojrXBYUQL45G",
    "outputId": "a9823272-05d5-49b1-8e45-b704d5f4304a"
   },
   "outputs": [],
   "source": [
    "#from google.colab import drive\n",
    "#drive.mount('/content/drive')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "324b6ee3",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-01-02T16:25:04.528790Z",
     "start_time": "2023-01-02T16:25:04.450757Z"
    },
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 757,
     "status": "ok",
     "timestamp": 1666692521312,
     "user": {
      "displayName": "蕭登方",
      "userId": "04110943827274540537"
     },
     "user_tz": -480
    },
    "id": "324b6ee3",
    "outputId": "6ec1f2ab-3356-4424-e2dd-b3b4ce81cf30"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "data len= 3513\n",
      "val len= 350\n"
     ]
    }
   ],
   "source": [
    "df_train=pd.read_csv(\"basic8_data_replace2.csv\")\n",
    "df_val=pd.read_csv(\"basic8_val.csv\")\n",
    "df_total=pd.read_csv(\"basic8_total.csv\")\n",
    "len_=len(df_train)\n",
    "movies=['寄生上流', '血觀音', '沙丘','返校','刻在你心底的名字','一級玩家','我的少女時代','天能']\n",
    "print(\"data len=\",len_)\n",
    "print(\"val len=\",len(df_val))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "04c55cd9",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-01-02T16:25:04.560753Z",
     "start_time": "2023-01-02T16:25:04.530756Z"
    },
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 371,
     "status": "ok",
     "timestamp": 1666692523971,
     "user": {
      "displayName": "蕭登方",
      "userId": "04110943827274540537"
     },
     "user_tz": -480
    },
    "id": "04c55cd9",
    "outputId": "b063a7ee-a9e7-4998-8622-61d6aca2e49e"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "after len= 2629\n",
      "after len= 303\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\user\\AppData\\Local\\Temp\\ipykernel_2916\\3786618029.py:6: UserWarning: Boolean Series key will be reindexed to match DataFrame index.\n",
      "  df_total = df_val[~(df_total.sentence.apply(lambda x : len(x)> MAX_LENGTH))]\n"
     ]
    }
   ],
   "source": [
    "#-------把太長的去掉------\n",
    "MAX_LENGTH=80\n",
    "\n",
    "df_train = df_train[~(df_train.sentence.apply(lambda x : len(x)> MAX_LENGTH))]\n",
    "df_val = df_val[~(df_val.sentence.apply(lambda x : len(x)> MAX_LENGTH))]\n",
    "df_total = df_val[~(df_total.sentence.apply(lambda x : len(x)> MAX_LENGTH))]\n",
    "#df_val_replace=df_val_replace[~(df_val_replace.sentence.apply(lambda x : len(x)> MAX_LENGTH))]\n",
    "len_=len(df_train)\n",
    "print(\"after len=\",len_)\n",
    "print(\"after len=\",len(df_val))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "ed31e792",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-01-02T16:25:04.590789Z",
     "start_time": "2023-01-02T16:25:04.566755Z"
    },
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 223
    },
    "executionInfo": {
     "elapsed": 301,
     "status": "ok",
     "timestamp": 1666692529866,
     "user": {
      "displayName": "蕭登方",
      "userId": "04110943827274540537"
     },
     "user_tz": -480
    },
    "id": "ed31e792",
    "outputId": "75712c05-a088-4019-947c-bb88ba6d7339"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "訓練樣本數： 2629\n",
      "                                            sentence label\n",
      "0                          網誌版，拖了好久，終於找到時間看這部獲獎無數的電影  寄生上流\n",
      "1                  故事情節懸疑緊湊，每個鏡頭、片段、，對話都精心設計與安排，名不虛傳  寄生上流\n",
      "2                而就像一杯美酒，好的藝術品除了本身的美感，更重，要的是品嘗過後那股後勁  寄生上流\n",
      "3  寄生上流是一部後勁非常強的電影，讓我們在關注完情節之後，，想到的是更多導演可能沒有明說，卻逼...  寄生上流\n",
      "4                      ，電影中最有名的台詞之一是媽媽說的不是『雖然有錢，卻很善良  寄生上流\n",
      "                                            sentence label\n",
      "0  金氏一家四口居於在首爾公寓的破舊半地下室中，全家因長期失業無正職工作而生活困頓，只能靠摺披薩...  寄生上流\n",
      "4             自此，金氏一家四口靠著裙帶關係及陰謀詭計，全數成功進佔朴家四份穩定的高薪肥缺  寄生上流\n",
      "5  數日後，朴家為慶祝多頌生日闔府外出露營度假，並託新管家忠淑留守看管，金家四口當晚於是暫駐大宅...  寄生上流\n",
      "6             然而雯光在雨夜忽然到訪門前按鈴，表示因走得匆忙而忘了取回地下室遺留的重要東西  寄生上流\n",
      "8                      此時突然天降豪雨，朴家被逼取消行程提前折返，令金家計畫失控  寄生上流\n"
     ]
    }
   ],
   "source": [
    "# idempotence, 將處理結果另存成 tsv 供 PyTorch 使用\n",
    "\n",
    "print(\"訓練樣本數：\", len(df_train))\n",
    "print(df_train.head())\n",
    "print(df_val.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "353f0af0",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-01-02T16:25:04.636755Z",
     "start_time": "2023-01-02T16:25:04.592756Z"
    },
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 290,
     "status": "ok",
     "timestamp": 1666692531993,
     "user": {
      "displayName": "蕭登方",
      "userId": "04110943827274540537"
     },
     "user_tz": -480
    },
    "id": "353f0af0",
    "outputId": "0187476e-f888-4736-f176-0dc94b094f9f"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "寄生上流        0.151769\n",
       "沙丘          0.150247\n",
       "血觀音         0.147965\n",
       "返校          0.138836\n",
       "刻在你心底的名字    0.133891\n",
       "一級玩家        0.107265\n",
       "我的少女時代      0.089768\n",
       "天能          0.080259\n",
       "Name: label, dtype: float64"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_train.label.value_counts() / len(df_train)\n",
    "df_train.to_csv(\"train.tsv\", sep=\"\\t\", index=False)\n",
    "df_train.label.value_counts() / len(df_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "2accd4ec",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-01-02T16:25:04.651755Z",
     "start_time": "2023-01-02T16:25:04.638755Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "血觀音         0.303630\n",
       "天能          0.244224\n",
       "我的少女時代      0.105611\n",
       "沙丘          0.079208\n",
       "寄生上流        0.072607\n",
       "刻在你心底的名字    0.072607\n",
       "返校          0.062706\n",
       "一級玩家        0.059406\n",
       "Name: label, dtype: float64"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_val.to_csv(\"val.tsv\", sep=\"\\t\", index=False)\n",
    "df_val.label.value_counts() / len(df_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "1fae5cbb",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-01-02T16:25:04.667753Z",
     "start_time": "2023-01-02T16:25:04.654753Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\npercentage=df_val.label.value_counts() / len(df_val)\\nwhile max(percentage)>0.20:\\n    for serie in series:\\n        if percentage[serie]>0.20:\\n            bad_label_index=df_val[df_val[\"label\"]==serie].sample(len(df_val)//20).index\\n            df_val = df_val[~df_val.index.isin(bad_label_index)]\\n    percentage=df_val.label.value_counts() / len(df_val)\\nprint(percentage)\\n\\n\\ndf_val.to_csv(\"val.tsv\", sep=\"\\t\", index=False)\\n'"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#if data is unbalance https://stackoverflow.com/questions/57244781/deleting-large-amount-of-data-from-pandas-dataframe\n",
    "\"\"\"\n",
    "bad_label_index=df_val[df_val[\"label\"]==\"血觀音\"].sample(60).index\n",
    "df_val = df_val[~df_val.index.isin(bad_label_index)]\n",
    "t=df_val.index.isin(bad_label_index)\n",
    "\"\"\"\n",
    "\"\"\"\n",
    "percentage=df_val.label.value_counts() / len(df_val)\n",
    "while max(percentage)>0.20:\n",
    "    for serie in series:\n",
    "        if percentage[serie]>0.20:\n",
    "            bad_label_index=df_val[df_val[\"label\"]==serie].sample(len(df_val)//20).index\n",
    "            df_val = df_val[~df_val.index.isin(bad_label_index)]\n",
    "    percentage=df_val.label.value_counts() / len(df_val)\n",
    "print(percentage)\n",
    "\n",
    "\n",
    "df_val.to_csv(\"val.tsv\", sep=\"\\t\", index=False)\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "14704ea5",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-01-02T16:25:04.683755Z",
     "start_time": "2023-01-02T16:25:04.669755Z"
    }
   },
   "outputs": [],
   "source": [
    "#if data is unbalance https://stackoverflow.com/questions/57244781/deleting-large-amount-of-data-from-pandas-dataframe\n",
    "\"\"\"\n",
    "bad_label_index=df_val[df_val[\"label\"]==\"血觀音\"].sample(60).index\n",
    "df_val = df_val[~df_val.index.isin(bad_label_index)]\n",
    "t=df_val.index.isin(bad_label_index)\n",
    "\"\"\"\n",
    "\"\"\"\n",
    "percentage=df_total.label.value_counts() / len(df_total)\n",
    "while max(percentage)>0.20:\n",
    "    for serie in series:\n",
    "        if percentage[serie]>0.20:\n",
    "            bad_label_index=df_total[df_total[\"label\"]==serie].sample(len(df_total)//20).index\n",
    "            df_total = df_total[~df_total.index.isin(bad_label_index)]\n",
    "    percentage=df_total.label.value_counts() / len(df_total)\n",
    "print(percentage)\n",
    "\"\"\"\n",
    "df_total.to_csv(\"total.tsv\", sep=\"\\t\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07c9d6a4",
   "metadata": {
    "id": "07c9d6a4"
   },
   "source": [
    "tokens_tensor：代表識別每個 token 的索引值，用 tokenizer 轉換即可  \n",
    "\n",
    "segments_tensor：用來識別句子界限。第一句所有index對應 0，第二句所有index對應 1。另外注意句子間的 [SEP] 為 0  \n",
    "\n",
    "masks_tensor：用來界定自注意力機制範圍。1 讓 BERT 關注該位置，0 則代表是 padding 不需關注  //因為我們會zero padding讓batch裡大小一致\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "t91Y61Ewg67p",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-01-02T16:25:04.699754Z",
     "start_time": "2023-01-02T16:25:04.685754Z"
    },
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 397,
     "status": "ok",
     "timestamp": 1666702388532,
     "user": {
      "displayName": "蕭登方",
      "userId": "04110943827274540537"
     },
     "user_tz": -480
    },
    "id": "t91Y61Ewg67p",
    "outputId": "c2834a1f-f13f-4ea0-8a69-405e7d60fd60"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{0: '寄生上流', 1: '血觀音', 2: '沙丘', 3: '返校', 4: '刻在你心底的名字', 5: '一級玩家', 6: '我的少女時代', 7: '天能'}\n",
      "['沙丘', '返校', '刻在你心底的名字']\n"
     ]
    }
   ],
   "source": [
    "label_map = {movie:i  for i, movie in enumerate(movies)}\n",
    "inv_label_map = {v: k for k, v in label_map.items()}\n",
    "print(inv_label_map)\n",
    "\n",
    "\n",
    "def indices_to_labels(indices):\n",
    "    for i in range(len(indices)):\n",
    "        indices[i] = inv_label_map[indices[i]]\n",
    "    return indices\n",
    "\n",
    "\n",
    "print(indices_to_labels([2, 3, 4]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "898872a1",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-01-02T16:25:04.715754Z",
     "start_time": "2023-01-02T16:25:04.701754Z"
    },
    "executionInfo": {
     "elapsed": 1,
     "status": "ok",
     "timestamp": 1666692545270,
     "user": {
      "displayName": "蕭登方",
      "userId": "04110943827274540537"
     },
     "user_tz": -480
    },
    "id": "898872a1"
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "實作一個可以用來讀取訓練 / 測試集的 Dataset，這是你需要徹底了解的部分。\n",
    "此 Dataset 每次將 tsv 裡的一筆成對句子轉換成 BERT 相容的格式，並回傳 3 個 tensors：\n",
    "- tokens_tensor：兩個句子合併後的索引序列，包含 [CLS] 與 [SEP]\n",
    "- segments_tensor：可以用來識別兩個句子界限的 binary tensor\n",
    "- label_tensor：將分類標籤轉換成類別索引的 tensor, 如果是測試集則回傳 None\n",
    "\"\"\"\n",
    "from torch.utils.data import Dataset\n",
    "\n",
    "#https://pytorch.org/tutorials/beginner/data_loading_tutorial.html\n",
    "class SeriesDataset(Dataset):\n",
    "    # 讀取前處理後的 tsv 檔並初始化一些參數\n",
    "    def __init__(self, mode, tokenizer,label_map):\n",
    "        #assert mode in [\"train\", \"test\",\"val\"]  # 確定檔案名稱只接受train或test\n",
    "        self.mode = mode\n",
    "        # 大數據你會需要用 iterator=True\n",
    "        self.df = pd.read_csv(mode + \".tsv\", sep=\"\\t\").fillna(\"\")   #讀處裡好的tsv避免跟所有檔案的csv搞混\n",
    "        self.len = len(self.df)\n",
    "        self.label_map = label_map\n",
    "        self.tokenizer = tokenizer  # 我們將使用 BERT tokenizer\n",
    "\n",
    "    # 定義回傳一筆訓練 / 測試數據的函式\n",
    "    def __getitem__(self, idx):\n",
    "        \"\"\"\n",
    "        if self.mode == \"test\":\n",
    "            text, label = self.df.iloc[idx, :].values\n",
    "            label_tensor = None\n",
    "        else:\n",
    "        \"\"\"\n",
    "        text,label = self.df.iloc[idx, :].values\n",
    "        # 將 label 文字也轉換成索引方便轉換成 tensor\n",
    "        label_id = self.label_map[label]\n",
    "        label_tensor = torch.tensor(label_id)\n",
    "\n",
    "        # 建立第一個句子的 BERT tokens 並加入分隔符號 [SEP]\n",
    "        word_pieces = [\"[CLS]\"]\n",
    "        tokens = self.tokenizer.tokenize(text)\n",
    "        word_pieces += tokens + [\"[SEP]\"]\n",
    "        \n",
    "        # 將整個 token 序列轉換成索引序列\n",
    "        ids = self.tokenizer.convert_tokens_to_ids(word_pieces)\n",
    "        tokens_tensor = torch.tensor(ids)\n",
    "\n",
    "        return (tokens_tensor, label_tensor)\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.len"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "d918f374",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-01-02T16:25:04.762754Z",
     "start_time": "2023-01-02T16:25:04.719754Z"
    },
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 302,
     "status": "ok",
     "timestamp": 1666692548355,
     "user": {
      "displayName": "蕭登方",
      "userId": "04110943827274540537"
     },
     "user_tz": -480
    },
    "id": "d918f374",
    "outputId": "0d26e54e-7846-400e-cbca-b14508b7da22"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                               sentence label\n",
      "0                             網誌版，拖了好久，終於找到時間看這部獲獎無數的電影  寄生上流\n",
      "1                     故事情節懸疑緊湊，每個鏡頭、片段、，對話都精心設計與安排，名不虛傳  寄生上流\n",
      "2                   而就像一杯美酒，好的藝術品除了本身的美感，更重，要的是品嘗過後那股後勁  寄生上流\n",
      "3     寄生上流是一部後勁非常強的電影，讓我們在關注完情節之後，，想到的是更多導演可能沒有明說，卻逼...  寄生上流\n",
      "4                         ，電影中最有名的台詞之一是媽媽說的不是『雖然有錢，卻很善良  寄生上流\n",
      "...                                                 ...   ...\n",
      "2624  ，那意味著這是一種雞生蛋、蛋生雞的問題，你所觀察到的整段時間長度只能是濃縮成一個，點，才可能...    天能\n",
      "2625  ，但在同時發生的狀況下，必然不可能逆行時間，因為一旦逆行，意味著你跟其他事件已經，脫鉤，不符...    天能\n",
      "2626  ，那要能逆行時間，你只能破壞掉命定論，若是破壞線性時間，因為本作的命定論並非用收，束來解釋，...    天能\n",
      "2627              ，所以要同時成立本作三個設定，命定論 + 線性時間 +可逆行時間必然不成立    天能\n",
      "2628                               ，所以還是不要在意這部的時空理論吧‧‧‧    天能\n",
      "\n",
      "[2629 rows x 2 columns]\n",
      "                                              sentence label\n",
      "0    金氏一家四口居於在首爾公寓的破舊半地下室中，全家因長期失業無正職工作而生活困頓，只能靠摺披薩...  寄生上流\n",
      "1               自此，金氏一家四口靠著裙帶關係及陰謀詭計，全數成功進佔朴家四份穩定的高薪肥缺  寄生上流\n",
      "2    數日後，朴家為慶祝多頌生日闔府外出露營度假，並託新管家忠淑留守看管，金家四口當晚於是暫駐大宅...  寄生上流\n",
      "3               然而雯光在雨夜忽然到訪門前按鈴，表示因走得匆忙而忘了取回地下室遺留的重要東西  寄生上流\n",
      "4                        此時突然天降豪雨，朴家被逼取消行程提前折返，令金家計畫失控  寄生上流\n",
      "..                                                 ...   ...\n",
      "298  為隱藏演算機下落，原先想滅口主角與尼爾的艾弗斯決定放他們一馬，將演算機拆成三份，讓知道真相的...    天能\n",
      "299  尼爾卻突然將他的那份組件交給主角，表示自己還有一個回到過去的任務，主角注意到尼爾背包上的紅繩...    天能\n",
      "300        在事件平安落幕後，某天凱特到學校接兒子放學時卻發覺不太對勁，她立即用那支手機留言給主角    天能\n",
      "301  此時普莉亞正打算將知道太多TENET天能機密的凱特滅口，不過主角在未來收到求救訊息後，透過逆...    天能\n",
      "302                    並揭露原來自始至終創建並領導TENET天能組織的人，都是他自己    天能\n",
      "\n",
      "[303 rows x 2 columns]\n"
     ]
    }
   ],
   "source": [
    "# 初始化一個專門讀取訓練樣本的 Dataset，使用中文 BERT 斷詞\n",
    "trainset = SeriesDataset(\"train\", tokenizer=tokenizer,label_map=label_map)     #tokenizer 在上面注意力圖 上有宣告\n",
    "valset=SeriesDataset(\"val\", tokenizer=tokenizer,label_map=label_map)\n",
    "totalset=SeriesDataset(\"total\", tokenizer=tokenizer,label_map=label_map)\n",
    "#valset_replace = SeriesDataset(\"val_replace\", tokenizer=tokenizer,label_map=label_map)     #tokenizer 在上面注意力圖 上有宣告\n",
    "print(trainset.df)\n",
    "print(valset.df)\n",
    "#print(valset_replace.df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "4be87f75",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-01-02T16:25:04.778754Z",
     "start_time": "2023-01-02T16:25:04.765755Z"
    },
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 290,
     "status": "ok",
     "timestamp": 1666695853814,
     "user": {
      "displayName": "蕭登方",
      "userId": "04110943827274540537"
     },
     "user_tz": -480
    },
    "id": "4be87f75",
    "outputId": "eb451851-5af6-4c91-f2a6-696a1d7dec88"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2371\n",
      "[原始文本]\n",
      "句子 1：即便遍，體麟傷，你的對抗直來直往，坦坦蕩蕩\n",
      "分類  ：我的少女時代\n",
      "\n",
      "--------------------\n",
      "\n",
      "[Dataset 回傳的 tensors]\n",
      "tokens_tensor  ：tensor([ 101, 1315,  912, 6881, 8024, 7768, 7929, 1003, 8024,  872, 4638, 2205,\n",
      "        2834, 4684,  889, 4684, 2518, 8024, 1788, 1788, 5939, 5939,  102])\n",
      "\n",
      "label_tensor   ：6\n",
      "--------------------\n",
      "\n",
      "[還原 tokens_tensors]\n",
      "[CLS]即便遍，體麟傷，你的對抗直來直往，坦坦蕩蕩[SEP]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "# 選擇一個樣本\n",
    "sample_idx = int(random.random()*len(trainset))\n",
    "print(sample_idx)\n",
    "# 將原始文本拿出做比較\n",
    "text,label = trainset.df.iloc[sample_idx]\n",
    "\n",
    "# 利用剛剛建立的 Dataset 取出轉換後的 id tensors\n",
    "tokens_tensor,label_tensor = trainset[sample_idx]\n",
    "\n",
    "# 將 tokens_tensor 還原成文本\n",
    "tokens = tokenizer.convert_ids_to_tokens(tokens_tensor.tolist())\n",
    "combined_text = \"\".join(tokens)     #tokens 是一個很多字元的list用.join把他轉成string\n",
    "\n",
    "# 渲染前後差異，毫無反應就是個 print。可以直接看輸出結果\n",
    "print(f\"\"\"[原始文本]\n",
    "句子 1：{text}\n",
    "分類  ：{label}\n",
    "\n",
    "--------------------\n",
    "\n",
    "[Dataset 回傳的 tensors]\n",
    "tokens_tensor  ：{tokens_tensor}\n",
    "\n",
    "label_tensor   ：{label_tensor}\n",
    "--------------------\n",
    "\n",
    "[還原 tokens_tensors]\n",
    "{combined_text}\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "d55c40bd",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-01-02T16:25:04.794754Z",
     "start_time": "2023-01-02T16:25:04.780755Z"
    },
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 292,
     "status": "ok",
     "timestamp": 1666692552162,
     "user": {
      "displayName": "蕭登方",
      "userId": "04110943827274540537"
     },
     "user_tz": -480
    },
    "id": "d55c40bd",
    "outputId": "83ad9346-ce24-4b96-a727-cb22ba635434"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'tuple'>\n",
      "2629\n",
      "<class '__main__.SeriesDataset'>\n"
     ]
    }
   ],
   "source": [
    "print(type(trainset[0]))\n",
    "print(len(trainset))\n",
    "print(type(trainset))\n",
    "\n",
    "#trainset 直接用[]來看是一個(2657,)個tuple of three tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "a4ecf92e",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-01-02T16:25:04.810753Z",
     "start_time": "2023-01-02T16:25:04.797754Z"
    },
    "executionInfo": {
     "elapsed": 333,
     "status": "ok",
     "timestamp": 1666692553795,
     "user": {
      "displayName": "蕭登方",
      "userId": "04110943827274540537"
     },
     "user_tz": -480
    },
    "id": "a4ecf92e"
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "實作可以一次回傳一個 mini-batch 的 DataLoader\n",
    "這個 DataLoader 吃我們上面定義的 `MovieDataset`，\n",
    "回傳訓練 BERT 時會需要的 4 個 tensors：\n",
    "- tokens_tensors  : (batch_size, max_seq_len_in_batch)\n",
    "- segments_tensors: (batch_size, max_seq_len_in_batch)\n",
    "- masks_tensors   : (batch_size, max_seq_len_in_batch)\n",
    "- label_ids       : (batch_size)\n",
    "\"\"\"\n",
    "\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.nn.utils.rnn import pad_sequence\n",
    "\n",
    "# 這個函式的輸入 `samples` 是一個 list由前面的3個tenors組成，裡頭的每個 element 都是\n",
    "# 剛剛定義的 `MovieDataset` 回傳的一個樣本，每個樣本都包含 3 tensors：\n",
    "# - tokens_tensor\n",
    "# - segments_tensor\n",
    "# - label_tensor\n",
    "# 它會對前兩個 tensors 作 zero padding，並產生前面說明過的 masks_tensors\n",
    "def create_mini_batch(samples):\n",
    "    tokens_tensors = [s[0] for s in samples]       #把每個sample的tensor取出來放進list 產生一個 list of tensor (batch_size,)\n",
    "    \n",
    "    # 測試集有 labels\n",
    "    if samples[0][1] is not None:\n",
    "        label_ids = torch.stack([s[1] for s in samples])\n",
    "    else:\n",
    "        label_ids = None\n",
    "    \n",
    "    # zero pad 到同一序列長度(要同時做好幾個RNN就要batch中每個元素一樣長)\n",
    "    #https://blog.csdn.net/qq_43391414/article/details/123289492\n",
    "    \n",
    "    tokens_tensors = pad_sequence(tokens_tensors, \n",
    "                                  batch_first=True)\n",
    "    \n",
    "    return tokens_tensors, label_ids\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "0f100f90",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-01-02T16:25:04.825753Z",
     "start_time": "2023-01-02T16:25:04.812754Z"
    },
    "executionInfo": {
     "elapsed": 419,
     "status": "ok",
     "timestamp": 1666695647974,
     "user": {
      "displayName": "蕭登方",
      "userId": "04110943827274540537"
     },
     "user_tz": -480
    },
    "id": "0f100f90"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nvalloader_replace = DataLoader(valset_replace, batch_size=BATCH_SIZE,\\n                         shuffle=True,\\n                         collate_fn=create_mini_batch\\n                         )\\n'"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#把data跟寫好的create_mini_batch放進trainloader\n",
    "BATCH_SIZE = 32\n",
    "trainloader = DataLoader(trainset, batch_size=BATCH_SIZE,\n",
    "                         shuffle=True,\n",
    "                         collate_fn=create_mini_batch\n",
    "                         )\n",
    "\n",
    "valloader = DataLoader(valset, batch_size=8,\n",
    "                         shuffle=True,\n",
    "                         collate_fn=create_mini_batch\n",
    "                         )\n",
    "totalloader = DataLoader(valset, batch_size=8,\n",
    "                         shuffle=True,\n",
    "                         collate_fn=create_mini_batch\n",
    "                         )\n",
    "\"\"\"\n",
    "valloader_replace = DataLoader(valset_replace, batch_size=BATCH_SIZE,\n",
    "                         shuffle=True,\n",
    "                         collate_fn=create_mini_batch\n",
    "                         )\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "7e51f4db",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-01-02T16:25:04.857756Z",
     "start_time": "2023-01-02T16:25:04.827754Z"
    },
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 454,
     "status": "ok",
     "timestamp": 1666692557603,
     "user": {
      "displayName": "蕭登方",
      "userId": "04110943827274540537"
     },
     "user_tz": -480
    },
    "id": "7e51f4db",
    "outputId": "814b03b2-a1ae-441c-8bf8-5dd654a39557"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "tokens_tensors.shape   = torch.Size([32, 82]) \n",
      "tensor([[ 101, 7440, 3152,  ...,    0,    0,    0],\n",
      "        [ 101, 8024, 1762,  ..., 1377,  809,  102],\n",
      "        [ 101, 8024, 2769,  ...,    0,    0,    0],\n",
      "        ...,\n",
      "        [ 101,  852, 3291,  ...,    0,    0,    0],\n",
      "        [ 101, 8024, 1008,  ...,    0,    0,    0],\n",
      "        [ 101, 4511, 3301,  ...,    0,    0,    0]])\n",
      "------------------------\n",
      "label_ids.shape        = torch.Size([32])\n",
      "tensor([3, 3, 4, 3, 4, 3, 4, 3, 4, 2, 2, 4, 1, 5, 0, 4, 2, 3, 5, 5, 4, 5, 7, 3,\n",
      "        4, 0, 0, 1, 1, 0, 5, 1])\n",
      "\n"
     ]
    }
   ],
   "source": [
    "data = next(iter(trainloader))  # 取出traing loader 裡的一個mini_batch\n",
    "\n",
    "tokens_tensors, label_ids = data\n",
    "\n",
    "print(f\"\"\"\n",
    "tokens_tensors.shape   = {tokens_tensors.shape} \n",
    "{tokens_tensors}\n",
    "------------------------\n",
    "label_ids.shape        = {label_ids.shape}\n",
    "{label_ids}\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec19265d",
   "metadata": {
    "id": "ec19265d"
   },
   "source": [
    "建立 BERT 用的 mini-batch 時最需要注意的就是 zero padding 的存在了。  \n",
    "你可以發現除了 lable_ids 以外，其他 3 個 tensors 的每個樣本的最後大都為 0，  \n",
    "這是因為每個樣本的 tokens 序列基本上長度都會不同，需要補 padding。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "05f6a28d",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-01-02T16:25:04.904753Z",
     "start_time": "2023-01-02T16:25:04.859753Z"
    },
    "executionInfo": {
     "elapsed": 3,
     "status": "ok",
     "timestamp": 1666692559822,
     "user": {
      "displayName": "蕭登方",
      "userId": "04110943827274540537"
     },
     "user_tz": -480
    },
    "id": "05f6a28d"
   },
   "outputs": [],
   "source": [
    "from transformers import BertForSequenceClassification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "3dc3c895",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-01-02T17:10:14.578212Z",
     "start_time": "2023-01-02T17:10:01.825437Z"
    },
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 2443,
     "status": "ok",
     "timestamp": 1666692563398,
     "user": {
      "displayName": "蕭登方",
      "userId": "04110943827274540537"
     },
     "user_tz": -480
    },
    "id": "3dc3c895",
    "outputId": "599259b5-efec-4e99-a1da-f6b5f5063df3"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at ckiplab/bert-base-chinese were not used when initializing BertForSequenceClassification: ['cls.predictions.decoder.bias', 'cls.predictions.decoder.weight', 'cls.predictions.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.dense.weight']\n",
      "- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at ckiplab/bert-base-chinese and are newly initialized: ['bert.pooler.dense.weight', 'classifier.weight', 'bert.pooler.dense.bias', 'classifier.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=====>LOAD checkpoint\n"
     ]
    },
    {
     "ename": "KeyError",
     "evalue": "'step'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_2916\\831420982.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     14\u001b[0m     \u001b[0mcheckpoint\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mload\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mPATH\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     15\u001b[0m     \u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mload_state_dict\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcheckpoint\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'model_state_dict'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 16\u001b[1;33m     \u001b[0moptimizer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mload_state_dict\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcheckpoint\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'optimizer_state_dict'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     17\u001b[0m \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     18\u001b[0m     \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"======>LOAD new model\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\envs\\tensorflow\\lib\\site-packages\\torch\\optim\\optimizer.py\u001b[0m in \u001b[0;36mload_state_dict\u001b[1;34m(self, state_dict)\u001b[0m\n\u001b[0;32m    212\u001b[0m         param_groups = [\n\u001b[0;32m    213\u001b[0m             update_group(g, ng) for g, ng in zip(groups, saved_groups)]\n\u001b[1;32m--> 214\u001b[1;33m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__setstate__\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m{\u001b[0m\u001b[1;34m'state'\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mstate\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'param_groups'\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mparam_groups\u001b[0m\u001b[1;33m}\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    215\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    216\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mzero_grad\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mset_to_none\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mbool\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mFalse\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\envs\\tensorflow\\lib\\site-packages\\torch\\optim\\adam.py\u001b[0m in \u001b[0;36m__setstate__\u001b[1;34m(self, state)\u001b[0m\n\u001b[0;32m     98\u001b[0m             \u001b[0mgroup\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msetdefault\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'capturable'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;32mFalse\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     99\u001b[0m         \u001b[0mstate_values\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mlist\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstate\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 100\u001b[1;33m         \u001b[0mstep_is_tensor\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mstate_values\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m!=\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mand\u001b[0m \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mis_tensor\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mstate_values\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'step'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    101\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mstep_is_tensor\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    102\u001b[0m             \u001b[1;32mfor\u001b[0m \u001b[0ms\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mstate_values\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyError\u001b[0m: 'step'"
     ]
    }
   ],
   "source": [
    "import os\n",
    "PRETRAINED_MODEL_NAME = \"ckiplab/bert-base-chinese\"\n",
    "NUM_LABELS = len(label_map)\n",
    "PATH=\"model_sam80.pt\"\n",
    "\n",
    "model = BertForSequenceClassification.from_pretrained(\n",
    "        PRETRAINED_MODEL_NAME, num_labels=NUM_LABELS,attention_probs_dropout_prob=0.10,\n",
    "        hidden_dropout_prob=0.10)\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=1e-5)\n",
    "#uncommend to load model\n",
    "\n",
    "if os.path.isfile(PATH):\n",
    "    print(\"=====>LOAD checkpoint\")\n",
    "    checkpoint = torch.load(PATH)\n",
    "    model.load_state_dict(checkpoint['model_state_dict'])\n",
    "    optimizer.load_state_dict(checkpoint['optimizer_state_dict'])\n",
    "else:\n",
    "    print(\"======>LOAD new model\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "6603f3c6",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-01-02T16:25:10.865690Z",
     "start_time": "2023-01-02T16:25:10.851656Z"
    },
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 293,
     "status": "ok",
     "timestamp": 1666692566016,
     "user": {
      "displayName": "蕭登方",
      "userId": "04110943827274540537"
     },
     "user_tz": -480
    },
    "id": "6603f3c6",
    "outputId": "3b81ca91-806a-4988-ea43-152fb1e50c9e"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "name            module\n",
      "----------------------\n",
      "bert:embeddings\n",
      "bert:encoder\n",
      "bert:pooler\n",
      "dropout         Dropout(p=0.1, inplace=False)\n",
      "classifier      Linear(in_features=768, out_features=8, bias=True)\n"
     ]
    }
   ],
   "source": [
    "print(\"\"\"\n",
    "name            module\n",
    "----------------------\"\"\")\n",
    "for name, module in model.named_children():\n",
    "    if name == \"bert\":\n",
    "        for n, _ in module.named_children():\n",
    "            print(f\"{name}:{n}\")\n",
    "    else:\n",
    "        print(\"{:15} {}\".format(name, module))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "70c58cda",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-01-02T16:25:10.881658Z",
     "start_time": "2023-01-02T16:25:10.867669Z"
    },
    "executionInfo": {
     "elapsed": 469,
     "status": "ok",
     "timestamp": 1666692568445,
     "user": {
      "displayName": "蕭登方",
      "userId": "04110943827274540537"
     },
     "user_tz": -480
    },
    "id": "70c58cda"
   },
   "outputs": [],
   "source": [
    "\"\"\"\"\n",
    "定義一個可以針對特定 DataLoader 取得模型預測結果以及分類準確度的函式\n",
    "之後也可以用來生成上傳到 Kaggle 競賽的預測結果\n",
    "\n",
    "2019/11/22 更新：在將 `tokens`、`segments_tensors` 等 tensors\n",
    "丟入模型時，強力建議指定每個 tensor 對應的參數名稱，以避免 HuggingFace\n",
    "更新 repo 程式碼並改變參數順序時影響到我們的結果。\n",
    "\"\"\"\n",
    "\n",
    "def get_predictions(model, dataloader, compute_acc=False):\n",
    "    predictions = None\n",
    "    correct = 0\n",
    "    total = 0\n",
    "      \n",
    "    with torch.no_grad():\n",
    "        # 遍巡整個資料集\n",
    "        wrong_list=[]\n",
    "        for data in dataloader:\n",
    "            # 將所有 tensors 移到 GPU 上\n",
    "            if next(model.parameters()).is_cuda:     #檢查model是否在GPU\n",
    "                data = [t.to(\"cuda:0\") for t in data if t is not None]    #sentence 跟label tensor從tuple改放到cuda list\n",
    "            tokens_tensors= data[0]   #data 已經放在cuda了 如果沒有的話 應該會報錯\n",
    "            outputs = model(input_ids=tokens_tensors)\n",
    "            #ouputs 原本是 SequenceClassifierOutput class\n",
    "            #用outputs[0]可以取出他的tensor部分 shape=(64,3)\n",
    "            logits = outputs[0]\n",
    "            _, pred = torch.max(logits.data, 1)   #後面的如果是0回傳整column中最大的,1的話回傳整row最大的\n",
    " \n",
    "            # 用來計算訓練集的分類準確率\n",
    "            if compute_acc:\n",
    "                labels = data[1]\n",
    "                total += labels.size(0)\n",
    "                correct += (pred == labels).sum().item()   #.item把tensor 轉為純量\n",
    "                \n",
    "                cur=0\n",
    "                for i in (pred!=labels):\n",
    "                    if i==True:\n",
    "                        wrong_list.append((tokens_tensors[cur],pred[cur].item(),labels[cur].item()))\n",
    "                    cur+=1\n",
    "                \n",
    "            # 將當前 batch 記錄下來\n",
    "            if predictions is None:\n",
    "                predictions = pred\n",
    "            else:\n",
    "                predictions = torch.cat((predictions, pred))  #最後會形成一長串所有的prediction\n",
    "    \n",
    "    if compute_acc:\n",
    "        acc = correct / total\n",
    "        return wrong_list, acc\n",
    "    return predictions\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "ae0322f7",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-01-02T17:10:53.829230Z",
     "start_time": "2023-01-02T17:10:53.704232Z"
    },
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 13426,
     "status": "ok",
     "timestamp": 1666692583757,
     "user": {
      "displayName": "蕭登方",
      "userId": "04110943827274540537"
     },
     "user_tz": -480
    },
    "id": "ae0322f7",
    "outputId": "2190ad13-4d1e-438f-f9d3-210ff481a24c"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "device: cuda:0\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# 讓模型跑在 GPU 上並取得訓練集的分類準確率\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(\"device:\", device)\n",
    "model = model.to(device)     #把model移到devive 應該會是cuda:0\n",
    "acc_record =0.5 #set 0 to start saving model\n",
    "train_acc_list=[]\n",
    "val_acc_list=[]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "9223099a",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-01-02T17:11:31.882419Z",
     "start_time": "2023-01-02T17:11:14.281368Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "classification acc: 0.9539748953974896\n"
     ]
    }
   ],
   "source": [
    "\n",
    "_, acc = get_predictions(model, trainloader, compute_acc=True)\n",
    "print(\"classification acc:\", acc)\n",
    "#print(\"total prediction:\",_.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "a11f6fed",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-01-02T16:25:11.040656Z",
     "start_time": "2023-01-02T16:25:11.026668Z"
    },
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 17,
     "status": "ok",
     "timestamp": 1666692076797,
     "user": {
      "displayName": "蕭登方",
      "userId": "04110943827274540537"
     },
     "user_tz": -480
    },
    "id": "a11f6fed",
    "outputId": "fed9e16e-4708-434d-fc17-ccef91d0ece1"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([21128, 768])\n"
     ]
    }
   ],
   "source": [
    "#for data in trainloader:\n",
    "    #print(data[0].shape)\n",
    "print(next(model.parameters()).shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "51b77908",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-01-02T16:25:11.072690Z",
     "start_time": "2023-01-02T16:25:11.042659Z"
    },
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 290,
     "status": "ok",
     "timestamp": 1666692589755,
     "user": {
      "displayName": "蕭登方",
      "userId": "04110943827274540537"
     },
     "user_tz": -480
    },
    "id": "51b77908",
    "outputId": "037c73fd-ec79-40c8-8e77-c88675e4445f"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([32, 77])\n",
      "torch.Size([32])\n",
      "<class 'list'>\n",
      "<class 'tuple'>\n"
     ]
    }
   ],
   "source": [
    "data = next(iter(trainloader))    #並沒有紀錄iter 所以不會一直next下去 只會取trainloader的開頭 並取得next\n",
    "for t in data:\n",
    "    print(t.shape)\n",
    "tmp=[t.to(\"cuda:0\") for t in data if t is not None]\n",
    "print(type(tmp))\n",
    "print(type(data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "5492b65b",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-01-02T16:25:11.088694Z",
     "start_time": "2023-01-02T16:25:11.074655Z"
    }
   },
   "outputs": [],
   "source": [
    "from sam import SAM\n",
    "\n",
    "from utility.smooth_cross_entropy import smooth_crossentropy\n",
    "from utility.initialize import initialize\n",
    "from utility.step_lr import StepLR\n",
    "from utility.bypass_bn import enable_running_stats, disable_running_stats"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23b2fc8a",
   "metadata": {
    "id": "23b2fc8a"
   },
   "source": [
    "输入：\n",
    "    input_ids：训练集，torch.LongTensor类型，shape是[batch_size, sequence_length]\n",
    "    token_type_ids：可选项，当训练集是两句话时才有的。\n",
    "    attention_mask：可选项，当使用mask才有，可参考原论文。\n",
    "    labels：数据标签，torch.LongTensor类型，shape是[batch_size]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "b22edbe4",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-01-02T17:14:45.325656Z",
     "start_time": "2023-01-02T17:12:25.206799Z"
    },
    "code_folding": [],
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 253770,
     "status": "ok",
     "timestamp": 1666702331436,
     "user": {
      "displayName": "蕭登方",
      "userId": "04110943827274540537"
     },
     "user_tz": -480
    },
    "id": "b22edbe4",
    "outputId": "4464ba3f-d761-4c49-c558-e677edc54a00"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch=0 iteration=0 acc=0.8125\n",
      "epoch=0 iteration=10 acc=0.71875\n",
      "epoch=0 iteration=20 acc=0.875\n",
      "epoch=0 iteration=30 acc=0.78125\n",
      "epoch=0 iteration=40 acc=0.8125\n",
      "epoch=0 iteration=50 acc=0.84375\n",
      "epoch=0 iteration=60 acc=0.84375\n",
      "epoch=0 iteration=70 acc=0.84375\n",
      "epoch=0 iteration=80 acc=0.90625\n",
      "[epoch 1] loss: 18.587, acc: 0.808,val acc: 0.782\n",
      "Save Checkpoint to: model_sam78.pt\n",
      "epoch=1 iteration=0 acc=0.59375\n",
      "epoch=1 iteration=10 acc=0.71875\n",
      "epoch=1 iteration=20 acc=0.6875\n",
      "epoch=1 iteration=30 acc=0.90625\n",
      "epoch=1 iteration=40 acc=0.75\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<timed exec>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\envs\\tensorflow\\lib\\site-packages\\torch\\autograd\\grad_mode.py\u001b[0m in \u001b[0;36mdecorate_context\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     25\u001b[0m         \u001b[1;32mdef\u001b[0m \u001b[0mdecorate_context\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     26\u001b[0m             \u001b[1;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mclone\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 27\u001b[1;33m                 \u001b[1;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     28\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mcast\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mF\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdecorate_context\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     29\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\jupyter_notebook\\Tensorflow(python 3.9)\\Movie_prediction\\movie_SAM\\sam.py\u001b[0m in \u001b[0;36msecond_step\u001b[1;34m(self, zero_grad)\u001b[0m\n\u001b[0;32m     35\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbase_optimizer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m  \u001b[1;31m# do the actual \"sharpness-aware\" update\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     36\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 37\u001b[1;33m         \u001b[1;32mif\u001b[0m \u001b[0mzero_grad\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mzero_grad\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     38\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     39\u001b[0m     \u001b[1;33m@\u001b[0m\u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mno_grad\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\envs\\tensorflow\\lib\\site-packages\\torch\\optim\\optimizer.py\u001b[0m in \u001b[0;36mzero_grad\u001b[1;34m(self, set_to_none)\u001b[0m\n\u001b[0;32m    247\u001b[0m                                 \u001b[0mp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mgrad\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrequires_grad_\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;32mFalse\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    248\u001b[0m                             \u001b[1;32mif\u001b[0m \u001b[1;33m(\u001b[0m\u001b[1;32mnot\u001b[0m \u001b[0mforeach\u001b[0m \u001b[1;32mor\u001b[0m \u001b[0mp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mgrad\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mis_sparse\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 249\u001b[1;33m                                 \u001b[0mp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mgrad\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mzero_\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    250\u001b[0m                             \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    251\u001b[0m                                 \u001b[0mper_device_and_dtype_grads\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mgrad\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdevice\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mgrad\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdtype\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mgrad\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "# 訓練模式\n",
    "model.train()\n",
    "\n",
    "# 使用 Adam Optim 更新整個分類模型的參數\n",
    "#optimizer = torch.optim.Adam(model.parameters(), lr=1e-5) #1e-4 train不起來\n",
    "base_optimizer = torch.optim.SGD\n",
    "optimizer = SAM(model.parameters(), base_optimizer, rho=2.0, adaptive=True, lr=0.00003, momentum=0.9)\n",
    "#optimizer = SAM(model.parameters(), base_optimizer, rho=2.0, adaptive=True, lr=0.001, momentum=0.9)\n",
    "EPOCHS = 15\n",
    "FREQ=10\n",
    "#acc_record=0.5\n",
    "\n",
    "for epoch in range(EPOCHS):\n",
    "\n",
    "    running_loss = 0.0\n",
    "    running_correct=0\n",
    "    running_count=0\n",
    "    model.train()\n",
    "    \n",
    "    for i,data in enumerate(trainloader):\n",
    "\n",
    "        tokens_tensors, labels = [t.to(device) for t in data]\n",
    "\n",
    "        # 將參數梯度歸零\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        # first forward-backward step\n",
    "        enable_running_stats(model)\n",
    "        outputs = model(input_ids=tokens_tensors,\n",
    "                labels=labels)\n",
    "        logits=outputs[1]\n",
    "        \n",
    "        loss = smooth_crossentropy(logits, labels)\n",
    "        loss.mean().backward()\n",
    "        optimizer.first_step(zero_grad=True)\n",
    "\n",
    "        # second forward-backward step\n",
    "        disable_running_stats(model)     \n",
    "        outputs = model(input_ids=tokens_tensors,\n",
    "                labels=labels)\n",
    "        logits=outputs[1]\n",
    "        smooth_crossentropy(logits, labels).mean().backward()\n",
    "        optimizer.second_step(zero_grad=True)\n",
    "        \n",
    "        \n",
    "        _, pred = torch.max(logits.data, 1)   #後面的如果是0回傳整column中最大的,1的話回傳整row最大的\n",
    "        correct = (pred == labels).sum().item()   #.item把tensor 轉為純量\n",
    "        if i%FREQ==0:            \n",
    "            print(\"epoch={} iteration={} acc={}\".format(epoch,i,correct/labels.size(0)))\n",
    "\n",
    "        # 紀錄當前 batch loss\n",
    "        running_loss += loss.mean().item()\n",
    "        running_count+= labels.size(0)\n",
    "        running_correct+=correct\n",
    "    # 計算分類準確率\n",
    "    model.eval()\n",
    "    acc=running_correct/running_count\n",
    "    with torch.no_grad():\n",
    "        _, val_acc = get_predictions(model, valloader, compute_acc=True)\n",
    "        train_acc_list.append(acc)\n",
    "        val_acc_list.append(val_acc)\n",
    "        print('[epoch %d] loss: %.3f, acc: %.3f,val acc: %.3f'%\n",
    "              (epoch + 1, running_loss, acc, val_acc))\n",
    "        if val_acc > acc_record:\n",
    "            acc_record = val_acc\n",
    "            SAVE_PATH = \"model_sam\"+str(int(val_acc*100))+\".pt\"\n",
    "            torch.save({\n",
    "                'model_state_dict': model.state_dict(),\n",
    "                'optimizer_state_dict': optimizer.state_dict(),\n",
    "            }, SAVE_PATH)\n",
    "            print(\"Save Checkpoint to:\", SAVE_PATH)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "b7da0a32",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-01-02T17:15:45.976280Z",
     "start_time": "2023-01-02T17:15:31.860540Z"
    },
    "code_folding": [],
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 253770,
     "status": "ok",
     "timestamp": 1666702331436,
     "user": {
      "displayName": "蕭登方",
      "userId": "04110943827274540537"
     },
     "user_tz": -480
    },
    "id": "b22edbe4",
    "outputId": "4464ba3f-d761-4c49-c558-e677edc54a00"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch=0 iteration=0 acc=0.375\n",
      "epoch=0 iteration=10 acc=0.5\n",
      "epoch=0 iteration=20 acc=0.25\n",
      "epoch=0 iteration=30 acc=0.375\n",
      "[epoch 1] loss: 17.168, acc: 0.498,val acc: 0.858\n",
      "Save Checkpoint to: model_sam85.pt\n",
      "Wall time: 14.1 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "# 訓練模式\n",
    "model.train()\n",
    "\n",
    "# 使用 Adam Optim 更新整個分類模型的參數\n",
    "#optimizer = torch.optim.Adam(model.parameters(), lr=1e-5) #1e-4 train不起來\n",
    "base_optimizer = torch.optim.SGD\n",
    "optimizer = SAM(model.parameters(), base_optimizer, rho=2.0, adaptive=True, lr=0.00003, momentum=0.9)\n",
    "#optimizer = SAM(model.parameters(), base_optimizer, rho=2.0, adaptive=True, lr=0.001, momentum=0.9)\n",
    "EPOCHS = 1\n",
    "FREQ=10\n",
    "#acc_record=0.5\n",
    "\n",
    "for epoch in range(EPOCHS):\n",
    "\n",
    "    running_loss = 0.0\n",
    "    running_correct=0\n",
    "    running_count=0\n",
    "    model.train()\n",
    "    \n",
    "    for i,data in enumerate(totalloader):\n",
    "\n",
    "        tokens_tensors, labels = [t.to(device) for t in data]\n",
    "\n",
    "        # 將參數梯度歸零\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        # first forward-backward step\n",
    "        enable_running_stats(model)\n",
    "        outputs = model(input_ids=tokens_tensors,\n",
    "                labels=labels)\n",
    "        logits=outputs[1]\n",
    "        \n",
    "        loss = smooth_crossentropy(logits, labels)\n",
    "        loss.mean().backward()\n",
    "        optimizer.first_step(zero_grad=True)\n",
    "\n",
    "        # second forward-backward step\n",
    "        disable_running_stats(model)     \n",
    "        outputs = model(input_ids=tokens_tensors,\n",
    "                labels=labels)\n",
    "        logits=outputs[1]\n",
    "        smooth_crossentropy(logits, labels).mean().backward()\n",
    "        optimizer.second_step(zero_grad=True)\n",
    "        \n",
    "        \n",
    "        _, pred = torch.max(logits.data, 1)   #後面的如果是0回傳整column中最大的,1的話回傳整row最大的\n",
    "        correct = (pred == labels).sum().item()   #.item把tensor 轉為純量\n",
    "        if i%FREQ==0:            \n",
    "            print(\"epoch={} iteration={} acc={}\".format(epoch,i,correct/labels.size(0)))\n",
    "\n",
    "        # 紀錄當前 batch loss\n",
    "        running_loss += loss.mean().item()\n",
    "        running_count+= labels.size(0)\n",
    "        running_correct+=correct\n",
    "    # 計算分類準確率\n",
    "    model.eval()\n",
    "    acc=running_correct/running_count\n",
    "    with torch.no_grad():\n",
    "        _, val_acc = get_predictions(model, valloader, compute_acc=True)\n",
    "    train_acc_list.append(acc)\n",
    "    val_acc_list.append(val_acc)\n",
    "    print('[epoch %d] loss: %.3f, acc: %.3f,val acc: %.3f'%\n",
    "          (epoch + 1, running_loss, acc, val_acc))\n",
    "    if val_acc > acc_record:\n",
    "        acc_record = val_acc\n",
    "        SAVE_PATH = \"model_sam\"+str(int(val_acc*100))+\".pt\"\n",
    "        torch.save({\n",
    "            'model_state_dict': model.state_dict(),\n",
    "            'optimizer_state_dict': optimizer.state_dict(),\n",
    "        }, SAVE_PATH)\n",
    "        print(\"Save Checkpoint to:\", SAVE_PATH)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "f8d5fcfb",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-01-02T16:38:46.872901Z",
     "start_time": "2023-01-02T16:38:45.024767Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9834983498349835\n",
      "(tensor([ 101, 4500, 3067, 5437, 3582, 6845, 7431, 6204, 3080, 4412, 1842, 4638,\n",
      "        6971, 5507, 1469, 1134, 2617, 3172, 1300, 1894, 3298, 1394, 8024, 1086,\n",
      "        2823, 1168,  924, 5397, 1469, 4049, 6205, 1649, 1957, 1894, 8024, 1724,\n",
      "         943,  782,  671, 6629, 1343, 5648, 4777, 4955, 2792, 6719, 5966,  102,\n",
      "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "           0,    0,    0,    0], device='cuda:0'), 7, 2)\n",
      "[CLS]用撲翼機逃離襲擊現場的鄧肯和凱恩斯博士會合，再找到保羅和潔西嘉女士，四個人一起去舊研究所躲藏[SEP]\n",
      "predict:天能 true:沙丘\n",
      "[CLS]顯示兩人皆知道marc##o躲在畫室[SEP]\n",
      "predict:刻在你心底的名字 true:血觀音\n",
      "[CLS]主角與艾弗斯注意到洞穴天井有爆破裝置，並發現一名死亡的士兵與一扇鎖上的門，死者背包上繫著一條紅繩吊飾[SEP]\n",
      "predict:一級玩家 true:天能\n",
      "[CLS]一日早晨，方芮欣的母親李妙子懷疑丈夫方道勤有外遇，卻遭到方道勤家暴，情緒失控的李妙子不斷念佛，並向菩薩許下希望方道勤消失的願望[SEP]\n",
      "predict:寄生上流 true:返校\n",
      "[CLS]院長夫人說近期流行的buffet像是在乞討一樣，殊不知身後庭院裡其他賓客正用著buffet[SEP]\n",
      "predict:寄生上流 true:血觀音\n"
     ]
    }
   ],
   "source": [
    "#print wrong predict of val\n",
    "model.eval()\n",
    "wrong_list, acc = get_predictions(model, valloader, compute_acc=True)\n",
    "print(acc)\n",
    "print(wrong_list[0])\n",
    "for wrong in wrong_list:\n",
    "    tokens = tokenizer.convert_ids_to_tokens(wrong[0])\n",
    "    combined_text = \"\".join(tokens)     #tokens 是一個很多字元的list用.join把他轉成string\n",
    "    combined_text=combined_text.replace(\"[PAD]\",\"\")\n",
    "    print(combined_text)\n",
    "    print(\"predict:{} true:{}\".format(inv_label_map[wrong[1]],inv_label_map[wrong[2]]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "894cfc3a",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-01-02T16:38:49.763958Z",
     "start_time": "2023-01-02T16:38:46.874763Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<Figure size 1500x1000 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#plot\n",
    "#plt.xlim(0,2200 ) # 設定 x 軸座標範圍\n",
    "import matplotlib.pyplot as plt\n",
    "plt.ylim(0, 1) # 設定 y 軸座標範圍\n",
    "\n",
    "plt.xlabel('epoch', fontsize=\"10\") # 設定 x 軸標題內容及大小\n",
    "plt.ylabel('Accuracy', fontsize=\"10\") # 設定 y 軸標題內容及大小\n",
    "plt.title('Accuracy in use replace data (dropout=0.1)(8 movie prediction)', fontsize=\"18\") # 設定圖表標題內容及大小\n",
    "\n",
    "plt.plot(range(len(train_acc_list)),train_acc_list,marker='o', color='blue',label=\"Training Accuracy\")\n",
    "plt.plot(range(len(val_acc_list)),val_acc_list,marker='o', color='red',label=\"Validation Accuracy\")\n",
    "#for x,y in zip(range(len(val_acc_list)),val_acc_list): \n",
    "#    plt.text(x, y+0.01, \"({}, {:.3f})\".format(x,y))\n",
    "\n",
    "plt.legend()\n",
    "fig = plt.gcf()\n",
    "fig.set_size_inches(15, 10)\n",
    "plt.savefig(\"Movie Accuracy use replace data (dropout=0.1).png\",dpi=100)\n",
    "\n",
    "plt.clf()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "dfb53c83",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-01-02T17:21:56.965668Z",
     "start_time": "2023-01-02T17:21:56.931630Z"
    },
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 276,
     "status": "ok",
     "timestamp": 1666702441197,
     "user": {
      "displayName": "蕭登方",
      "userId": "04110943827274540537"
     },
     "user_tz": -480
    },
    "id": "dfb53c83",
    "outputId": "9046831c-11fb-4e36-aa22-a6f32e2a76a0"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CLS]在浴室裡做色色的事[SEP]\n",
      "['[CLS]', '在', '浴', '室', '裡', '做', '色', '色', '的', '事', '[SEP]']\n",
      "[101, 1762, 3861, 2147, 6174, 976, 5682, 5682, 4638, 752, 102]\n",
      "Top 1 (75%)：血觀音\n",
      "Top 2 ( 8%)：寄生上流\n",
      "Top 3 ( 7%)：我的少女時代\n",
      "Top 4 ( 2%)：一級玩家\n",
      "Top 5 ( 2%)：天能\n",
      "Top 6 ( 1%)：刻在你心底的名字\n",
      "Top 7 ( 1%)：返校\n",
      "Top 8 ( 0%)：沙丘\n"
     ]
    }
   ],
   "source": [
    "model.eval()\n",
    "test_text = \"[CLS]在浴室裡做色色的事[SEP]\"  # 盡量湊到6個字以上 ex:xx到底是甚麼\n",
    "tokens = tokenizer.tokenize(test_text)\n",
    "ids = tokenizer.convert_tokens_to_ids(tokens)\n",
    "\n",
    "id_tensor = torch.tensor([ids]).to(device)\n",
    "print(test_text)\n",
    "print(tokens[:])\n",
    "print(ids[:])\n",
    "k = 8\n",
    "with torch.no_grad():\n",
    "    outputs = model(input_ids=id_tensor)\n",
    "    prediction = outputs[0]\n",
    "    probs, indices = torch.topk(torch.softmax(prediction, -1), k)\n",
    "    probs = probs.tolist()\n",
    "    indices = indices.tolist()\n",
    "    probs = probs[0]\n",
    "    indices = indices[0]\n",
    "for i, (indice, p) in enumerate(zip(indices, probs), 1):\n",
    "    label = inv_label_map[indice]\n",
    "    print(\"Top {} ({:2}%)：{}\".format(i, int(p * 100), label[:10]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "dcc5eca4",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-01-02T16:38:49.810957Z",
     "start_time": "2023-01-02T16:38:49.796959Z"
    }
   },
   "outputs": [],
   "source": [
    "def MoviePrediction(text,model,tokenizer):\n",
    "    test_text=\"[CLS]\"+text+\"[SEP]\"   #盡量湊到6個字以上 ex: xx到底是甚麼\n",
    "    tokens = tokenizer.tokenize(test_text)\n",
    "    ids = tokenizer.convert_tokens_to_ids(tokens)\n",
    "\n",
    "    id_tensor = torch.tensor([ids]).to(device)\n",
    "    print(text)\n",
    "    print(ids[:])\n",
    "    k=8\n",
    "    with torch.no_grad():\n",
    "        outputs = model(input_ids=id_tensor)     \n",
    "        prediction = outputs[0]\n",
    "        probs, indices = torch.topk(torch.softmax(prediction, -1), k)\n",
    "        probs=probs.tolist()\n",
    "        indices=indices.tolist()\n",
    "        probs=probs[0]\n",
    "        indices=indices[0]\n",
    "    for i, (indice, p) in enumerate(zip(indices, probs), 1):\n",
    "        label=inv_label_map[indice]\n",
    "        print(\"Top {} ({:2}%)：{}\".format(i, int(p * 100), label[:10]), '...')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "118b0cb7",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-01-02T16:38:49.857957Z",
     "start_time": "2023-01-02T16:38:49.812958Z"
    }
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'MoviePrediction' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_2916\\3693508006.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[0mtext\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m\"託妹妹偽造名校的學校\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 4\u001b[1;33m \u001b[0mMoviePrediction\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mmodel\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mtokenizer\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m: name 'MoviePrediction' is not defined"
     ]
    }
   ],
   "source": [
    "#text=\"姐姐其實是自己的媽媽\"\n",
    "text=\"託妹妹偽造名校的學校\"\n",
    "\n",
    "MoviePrediction(text,model,tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f7daea5",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-01-02T16:38:49.868959Z",
     "start_time": "2023-01-02T16:38:49.868959Z"
    }
   },
   "outputs": [],
   "source": [
    "tokenizer.convert_tokens_to_ids(\"_\")"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3.9 (tensorflow)",
   "language": "python",
   "name": "tensorflow"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "oldHeight": 286.85,
   "position": {
    "height": "308.841px",
    "left": "559.193px",
    "right": "20px",
    "top": "81px",
    "width": "728px"
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "varInspector_section_display": "block",
   "window_display": false
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "0a90c336520a4b37a07310ee07752420": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": ""
     }
    },
    "40937cf4c5b140cb996312e4619444db": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "64dc892c4b3645fcb2a3ba97f06e032c": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_e78290c248d741ec8a503a37b24aa143",
      "max": 1,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_0a90c336520a4b37a07310ee07752420",
      "value": 0
     }
    },
    "8dfa31321da04c048f4be0b325369ca5": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_be822330c6c04df1ac3699bbf3a2760c",
      "placeholder": "​",
      "style": "IPY_MODEL_9c06c8fbf24647908c33f7e522429777",
      "value": " 0/0 [00:00&lt;?, ?it/s]"
     }
    },
    "99457cf987d347368ed223bab27555c6": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "9c06c8fbf24647908c33f7e522429777": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "a542890fb452487e8eb6ea03b000ab33": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "be822330c6c04df1ac3699bbf3a2760c": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "caac41f66b0142e28added42102acc2b": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_cc50d7437a1b434683d60275b51a16b7",
       "IPY_MODEL_64dc892c4b3645fcb2a3ba97f06e032c",
       "IPY_MODEL_8dfa31321da04c048f4be0b325369ca5"
      ],
      "layout": "IPY_MODEL_40937cf4c5b140cb996312e4619444db"
     }
    },
    "cc50d7437a1b434683d60275b51a16b7": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_a542890fb452487e8eb6ea03b000ab33",
      "placeholder": "​",
      "style": "IPY_MODEL_99457cf987d347368ed223bab27555c6",
      "value": ""
     }
    },
    "e78290c248d741ec8a503a37b24aa143": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": "20px"
     }
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
